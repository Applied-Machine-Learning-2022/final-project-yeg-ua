{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Applied-Machine-Learning-2022/final-project-yeg-ua/blob/yasser/new_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "copyright"
      },
      "source": [
        "#### Copyright 2020 Google LLC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "khlO4Bu21oZ4"
      },
      "outputs": [],
      "source": [
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "7-sLQH-Y5awy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlzIlBsScJJ_"
      },
      "source": [
        "# Video Classification with Pre-Trained Models Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTirVS4FWaPx"
      },
      "source": [
        "In this project we will import a pre-existing model that recognizes objects and use the model to identify those objects in a video. We'll edit the video to draw boxes around the identified object, and then we'll reassemble the video so the boxes are shown around objects in the video."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTVUYxPwcHhp"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdIOgOHP1ces"
      },
      "source": [
        "## Exercise 1: Coding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhTEOK1ZmqN8"
      },
      "source": [
        "You will process a video frame by frame, identify objects in each frame, and draw a bounding box with a label around each car in the video.\n",
        " \n",
        "Use the [SSD MobileNet V1 Coco](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf1_detection_zoo.md) (*ssd_mobilenet_v1_coco*) model. The video you'll process can be found [on Pixabay](https://pixabay.com/videos/cars-motorway-speed-motion-traffic-1900/). The 640x360 version of the video is smallest and easiest to handle, though any size should work since you must scale down the images for processing.\n",
        " \n",
        "Your program should:\n",
        " \n",
        "* Read in a video file (use the one in this colab if you want)\n",
        "* Load the TensorFlow model linked above\n",
        "* Loop over each frame of the video\n",
        "* Scale the frame down to a size the model expects\n",
        "* Feed the frame to the model\n",
        "* Loop over detections made by the model\n",
        "* If the detection score is above some threshold, draw a bounding box onto the frame and put a label in or near the box\n",
        "* Write the frame back to a new video\n",
        " \n",
        "Some tips:\n",
        " \n",
        "* Processing an entire video is slow, so consider truncating the video or skipping over frames during development. Skipping frames will make the video choppy. But you'll be able to see a wider variety of images than you would with a truncated video with all of the original frames in the clip.\n",
        "* The model expects a 300x300 image. You'll likely have to scale your frames to fit the model. When you get a bounding box, that box is relative to the scaled image. You'll need to scale the bounding box out to the original image size.\n",
        "* Don't start by trying to process the video. Instead, capture one frame and work with it until you are happy with your object detection, bounding boxes, and labels. Once you get those done, use the same logic on the other frames of the video.\n",
        "* The [Coco labels file](https://github.com/nightrome/cocostuff/blob/master/labels.txt) can be used to identify classified objects.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XM35vYWSbim"
      },
      "source": [
        "### **Student Solution**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "import os\n",
        "import tarfile\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "import cv2 as cv\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Rectangle\n",
        "base_url = 'http://download.tensorflow.org/models/object_detection/'    #download model\n",
        "file_name = 'ssd_mobilenet_v1_coco_2018_01_28.tar.gz'\n",
        "url = base_url + file_name\n",
        "urllib.request.urlretrieve(url, file_name)\n",
        "dir_name = file_name[0:-len('.tar.gz')]\n",
        "if os.path.exists(dir_name):\n",
        "  shutil.rmtree(dir_name)\n",
        "tarfile.open(file_name, 'r:gz').extractall('./')\n",
        "frozen_graph = os.path.join(dir_name, 'frozen_inference_graph.pb')\n",
        "with tf.io.gfile.GFile(frozen_graph, \"rb\") as f:\n",
        "  graph_def = tf.compat.v1.GraphDef()\n",
        "  loaded = graph_def.ParseFromString(f.read())\n",
        "\n",
        "\n",
        "def wrap_graph(graph_def, inputs, outputs, print_graph=False):\n",
        "  wrapped = tf.compat.v1.wrap_function(\n",
        "    lambda: tf.compat.v1.import_graph_def(graph_def, name=\"\"), [])\n",
        "  return wrapped.prune(\n",
        "    tf.nest.map_structure(wrapped.graph.as_graph_element, inputs),\n",
        "    tf.nest.map_structure(wrapped.graph.as_graph_element, outputs))\n",
        "  \n",
        "dict = {            #options\n",
        "0:\"background\",\n",
        "1:\"person\",\n",
        "2:\"bicycle\",\n",
        "3:\"car\",\n",
        "4:\"motorcycle\",\n",
        "5:\"airplane\",\n",
        "6:\"bus\",\n",
        "7:\"train\",\n",
        "8:\"truck\",\n",
        "9:\"boat\",\n",
        "10:\"trafficlight\",\n",
        "11:\"firehydrant\",\n",
        "12:\"unknown\",\n",
        "13:\"stopsign\",\n",
        "14:\"parkingmeter\",\n",
        "15:\"bench\",\n",
        "16:\"bird\",\n",
        "17:\"cat\",\n",
        "18:\"dog\",\n",
        "19:\"horse\",\n",
        "20:\"sheep\",\n",
        "21:\"cow\",\n",
        "22:\"elephant\",\n",
        "23:\"bear\",\n",
        "24:\"zebra\",\n",
        "25:\"giraffe\",\n",
        "26:\"unknown\",\n",
        "27:\"backpack\",\n",
        "28:\"umbrella\",\n",
        "29:\"unknown\",\n",
        "30:\"unknown\",\n",
        "31:\"handbag\",\n",
        "32:\"tie\",\n",
        "33:\"suitcase\",\n",
        "34:\"frisbee\",\n",
        "35:\"skis\",\n",
        "36:\"snowboard\",\n",
        "37:\"sportsball\",\n",
        "38:\"kite\",\n",
        "39:\"baseballbat\",\n",
        "40:\"baseballglove\",\n",
        "41:\"skateboard\",\n",
        "42:\"surfboard\",\n",
        "43:\"tennisracket\",\n",
        "44:\"bottle\",\n",
        "45:\"unknown\",\n",
        "46:\"wineglass\",\n",
        "47:\"cup\",\n",
        "48:\"fork\",\n",
        "49:\"knife\",\n",
        "50:\"spoon\",\n",
        "51:\"bowl\",\n",
        "52:\"banana\",\n",
        "53:\"apple\",\n",
        "54:\"sandwich\",\n",
        "55:\"orange\",\n",
        "56:\"broccoli\",\n",
        "57:\"carrot\",\n",
        "58:\"hotdog\",\n",
        "59:\"pizza\",\n",
        "60:\"donut\",\n",
        "61:\"cake\",\n",
        "62:\"chair\",\n",
        "63:\"couch\",\n",
        "64:\"pottedplant\",\n",
        "65:\"bed\",\n",
        "66:\"unknown\",\n",
        "67:\"diningtable\",\n",
        "68:\"unknown\",\n",
        "69:\"unknown\",\n",
        "70:\"toilet\",\n",
        "71:\"unknown\",\n",
        "72:\"tv\",\n",
        "73:\"laptop\",\n",
        "74:\"mouse\",\n",
        "75:\"remote\",\n",
        "76:\"keyboard\",\n",
        "77:\"cellphone\",\n",
        "78:\"microwave\",\n",
        "79:\"oven\",\n",
        "80:\"toaster\",\n",
        "81:\"sink\",\n",
        "82:\"refrigerator\",\n",
        "83:\"unknown\",\n",
        "84:\"book\",\n",
        "85:\"clock\",\n",
        "86:\"vase\",\n",
        "87:\"scissors\",\n",
        "88:\"teddybear\",\n",
        "89:\"hairdrier\"\n",
        "}\n",
        "\n",
        "\n",
        "def drawBoxes(frame):       #create box method\n",
        "    image = frame\n",
        "    outputs = (\n",
        "      'num_detections:0',\n",
        "      'detection_classes:0',\n",
        "      'detection_scores:0',\n",
        "      'detection_boxes:0',\n",
        "       )\n",
        "    input_images = [image]\n",
        "    model = wrap_graph(graph_def=graph_def,\n",
        "                  inputs=[\"image_tensor:0\" ],\n",
        "                   outputs=outputs)\n",
        "    tensor = tf.convert_to_tensor(input_images, dtype=tf.uint8)\n",
        "    detections = model(tensor)\n",
        "    boxes = []\n",
        "    i = 0\n",
        "    while detections[1][0][i].numpy().any():\n",
        "      boxes.append((detections[1][0][i].numpy(), detections[1][0][i].numpy()))\n",
        "      i += 1\n",
        "    height = image.shape[0]\n",
        "    width = image.shape[1]\n",
        "    for box in boxes:\n",
        "      label = box[1]\n",
        "      y1 = int(box[0][0] * height)\n",
        "      x1 = int(box[0][1] * width)\n",
        "      y2 = int(box[0][2] * height)\n",
        "      x2 = int(box[0][3] * width)\n",
        "      image = cv.rectangle(image, (x1, y1), (x2, y2), (0,255,0), 2)\n",
        "      cv.putText(image, dict[1], (x1,y2), cv.FONT_HERSHEY_SIMPLEX, .8, [0,0,0], 2)\n",
        "    return image\n",
        "input_video = cv.VideoCapture('people')         #grab regular car video\n",
        "height = int(input_video.get(cv.CAP_PROP_FRAME_HEIGHT))\n",
        "width = int(input_video.get(cv.CAP_PROP_FRAME_WIDTH))\n",
        "fps = input_video.get(cv.CAP_PROP_FPS)\n",
        "total_frames = int(input_video.get(cv.CAP_PROP_FRAME_COUNT))\n",
        "fourcc = cv.VideoWriter_fourcc(*'mp4v')\n",
        "output_video = cv.VideoWriter('people.mp4', fourcc, fps, (width, height))\n",
        "for i in range(0, total_frames, 1):\n",
        "  input_video.set(cv.CAP_PROP_POS_FRAMES, i)\n",
        "  ret, frames = input_video.read()\n",
        "  frames = drawBoxes(frames)\n",
        "  if not ret:\n",
        "    raise Exception(\"Problem reading frame\", i, \" from video\")\n",
        "  output_video.write(frames)\n",
        "input_video.release()   #release cars.mp4\n",
        "\n",
        "output_video.release()  #release cars-detected.mp4"
      ],
      "metadata": {
        "id": "pu_84K1qtxnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEGDiC-IhcrM"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HniKdSXg0YHR"
      },
      "source": [
        "## Exercise 2: Ethical Implications"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4FvC1Aa0ZT5"
      },
      "source": [
        "Even the most basic models have the potential to affect segments of the population in different ways. It is important to consider how your model might positively and negatively affect different types of users.\n",
        "\n",
        "In this section of the project, you will reflect on the positive and negative implications of your model. Frame the context of your model creation using this narrative:\n",
        "\n",
        "> The city of Seattle is attempting to reduce traffic congestion in its downtown area. As part of this project, they plan to allow each local driver one free trip to downtown Seattle per week. After that, the driver will have to pay a $50 toll for each extra day per week driven. As an early proof of concept for this project, your team is tasked with using machine learning to correctly identify automobiles on the road. The next phase of the project will involve detecting license plate numbers and then cross-referencing that data with RFID chips that should be mounted in all local drivers' cars."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkyzwVQr0brd"
      },
      "source": [
        "### **Student Solution**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy4I2vG60ebd"
      },
      "source": [
        "**Positive Impact**\n",
        "\n",
        "Your model is trying to solve a problem. Think about who will benefit from that problem being solved and write a brief narrative about how the model will help."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k59MK1Ah0fWy"
      },
      "source": [
        "Many people will benefit from the reduction of traffic congestion. The main benefactor for this model would be any driver traveling in the new, less congested roads. Other people who would also see a positive impact include workers in the city like police officers who will have less trouble regulating traffic. The city in its entirety will also see an indirect benefit as automobile emissions would be greatly reduced with this new system. Finally, public transportation could benefit from this since less cars will be on the road and more people will new a new avenue of traveling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzqkrLnk0hMU"
      },
      "source": [
        "**Negative Impact**\n",
        "\n",
        "Models rarely benefit everyone equally. Think about who might be negatively impacted by the predictions your model is making. This person(s) might not be directly using the model, but they might be impacted indirectly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hefa1JdP0kj3"
      },
      "source": [
        "Some negative impacts may arise, therefore not all parties will benefit from this new system. For example, drivers who work in downtown Seattle and do not want to use public transportation would be negatively impacted. Another group of people who could also be impacted negatively are local business owners stationed downtown. Due to less congestion their businessâ€™s could see a decrease in traffic because people do not have a way of reaching them without having to pay an additional fee after their first visit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uax2HAzd0mHX"
      },
      "source": [
        "**Bias**\n",
        "\n",
        "Models can be biased for many reasons. The bias can come from the data used to build the model (e.g., sampling, data collection methods, available sources) and/or from the interpretation of the predictions generated by the model.\n",
        "\n",
        "Think of at least two ways bias might have been introduced to your model and explain both below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bJGm-qs0oQV"
      },
      "source": [
        "Since the data being analyzed may not be clear, bias is inevitable when trying to account for multiple moving vehicles. For example, developing a system to automatically identify cars could appear lackluster if the model is not given enough information between car models. Therefor, a main problem that may arise is the incorrect identification of some motor vehicles. \n",
        "\n",
        "Another example of bias that may be encountered is the incorrect reading of a license plate. Since the license plate accounts as the identification of a car, one incorrect digit could lead to a different person being charged a fee because of a mistake made by the program. To midigate biases like these the best option for training this program would be to give it as much data as needed, or to try different camera angles, so the accuracy can be as best as possible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ybb1zAkC0p2e"
      },
      "source": [
        "**Changing the Dataset to Mitigate Bias**\n",
        "\n",
        "Having bias in your dataset is one of the primary ways in which bias is introduced to a machine learning model. Look back at the input data you fed to your model. Think about how you might change something about the data to reduce bias in your model.\n",
        "\n",
        "What change or changes could you make to reduce the bias in your dataset? Consider the data you have, how and where it was collected, and what other sources of data might be used to reduce bias.\n",
        "\n",
        "Write a summary of changes that could be made to your input data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFsnF4_h08DD"
      },
      "source": [
        "> *Since the data has potential bias A we can adjust the method we use to determine which car has already used their free downtown pass. One adjustment could be using each driver's VIN. As stated earleir, although using license plate numbers is feasible, one missed number could distort the entire process.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChEJbhXA02pW"
      },
      "source": [
        "**Changing the Model to Mitigate Bias**\n",
        "\n",
        "Is there any way to reduce bias by changing the model itself? This could include modifying algorithmic choices, tweaking hyperparameters, etc.\n",
        "\n",
        "Write a brief summary of changes you could make to help reduce bias in your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEAhgO_U0p8Y"
      },
      "source": [
        "> *Since the model has potential bias A, we can adjust the way in which vehicles are checked and admitted. As stated earlier, cameras may not correctly document everything that is required of a person's vehicle, but having designated booths, with someone checking each vehicle could be a better solution.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rShB5BQv0wix"
      },
      "source": [
        "**Mitigating Bias Downstream**\n",
        "\n",
        "Models make predictions. Downstream processes make decisions. What processes and/or rules should be in place for people and systems interpreting and acting on the results of your model to reduce bias? Describe these rules and/or processes below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C__BwBP-00HN"
      },
      "source": [
        "> *Since the predictions have potential bias A, we can adjust the criteria for who should and shouldn't have to undergo the newly proposed downtown criteria. For example, someone who works downtown should not have to participate in this model.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1L_4RNXphYtI"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "copyright",
        "s5UQCAU90N81"
      ],
      "name": "Copy of Video Classification With Pre-Trained Models Project",
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python [conda env:root] *",
      "language": "python",
      "name": "conda-root-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.16"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}